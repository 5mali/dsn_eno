{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 0:  TOKYO, 2008 \n",
      "Average Reward \t\t= -7.943\n",
      "Violation Counter \t= 366\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 1:  TOKYO, 2003 \n",
      "Average Reward \t\t= -7.664\n",
      "Violation Counter \t= 359\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.819\n",
      "Violation Counter \t= 82\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 3:  TOKYO, 2002 \n",
      "Average Reward \t\t= -0.333\n",
      "Violation Counter \t= 52\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 4:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.868\n",
      "Violation Counter \t= 63\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.091\n",
      "Violation Counter \t= 63\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.445\n",
      "Violation Counter \t= 84\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 7:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.368\n",
      "Violation Counter \t= 79\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 8:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.508\n",
      "Violation Counter \t= 100\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 9:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.488\n",
      "Violation Counter \t= 109\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 10:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.676\n",
      "Violation Counter \t= 135\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 11:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.807\n",
      "Violation Counter \t= 132\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 12:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.651\n",
      "Violation Counter \t= 134\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 13:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.869\n",
      "Violation Counter \t= 131\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "Average Reward \t\t= -2.304\n",
      "Violation Counter \t= 145\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 15:  TOKYO, 2001 \n",
      "Average Reward \t\t= -1.632\n",
      "Violation Counter \t= 113\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.003\n",
      "Violation Counter \t= 79\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 17:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.504\n",
      "Violation Counter \t= 60\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 18:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.582\n",
      "Violation Counter \t= 66\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 19:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.264\n",
      "Violation Counter \t= 48\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 20:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.149\n",
      "Violation Counter \t= 45\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 21:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.376\n",
      "Violation Counter \t= 52\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 22:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.357\n",
      "Violation Counter \t= 51\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.896\n",
      "Violation Counter \t= 70\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 24:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.379\n",
      "Violation Counter \t= 51\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 25:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.592\n",
      "Violation Counter \t= 58\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 26:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.257\n",
      "Violation Counter \t= 42\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 27:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.520\n",
      "Violation Counter \t= 65\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 28:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.775\n",
      "Violation Counter \t= 72\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 29:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.793\n",
      "Violation Counter \t= 72\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 30:  TOKYO, 2006 \n",
      "Average Reward \t\t= -1.219\n",
      "Violation Counter \t= 82\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 31:  TOKYO, 2002 \n",
      "Average Reward \t\t= -1.135\n",
      "Violation Counter \t= 91\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.598\n",
      "Violation Counter \t= 66\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 33:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.780\n",
      "Violation Counter \t= 72\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 34:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.759\n",
      "Violation Counter \t= 74\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 35:  TOKYO, 2002 \n",
      "Average Reward \t\t= -0.924\n",
      "Violation Counter \t= 82\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= -1.158\n",
      "Violation Counter \t= 93\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 37:  TOKYO, 2006 \n",
      "Average Reward \t\t= -1.439\n",
      "Violation Counter \t= 107\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 38:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.659\n",
      "Violation Counter \t= 67\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 39:  TOKYO, 2008 \n",
      "Average Reward \t\t= -1.017\n",
      "Violation Counter \t= 86\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 40:  TOKYO, 2006 \n",
      "Average Reward \t\t= -1.277\n",
      "Violation Counter \t= 95\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 41:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.809\n",
      "Violation Counter \t= 75\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 42:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.033\n",
      "Violation Counter \t= 92\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 43:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.890\n",
      "Violation Counter \t= 82\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 44:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.815\n",
      "Violation Counter \t= 74\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 45:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.816\n",
      "Violation Counter \t= 71\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.898\n",
      "Violation Counter \t= 92\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 47:  TOKYO, 2003 \n",
      "Average Reward \t\t= -1.094\n",
      "Violation Counter \t= 100\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 48:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.744\n",
      "Violation Counter \t= 80\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.814\n",
      "Violation Counter \t= 86\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "#     counter = iteration%10\n",
    "#     EPSILON = 0.5*counter/(counter+1) + 0.5#sawtooth learning rate for disruptive learning\n",
    "    print('EPSILON = {:.2}'.format(EPSILON))\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "#             transition_rec[:,5] += r #broadcast reward to all states\n",
    "#             decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "#             transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "   \n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.95\n",
      "LR      =  0.0001\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.452\n",
      "Violation Counter \t= 57\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.477\n",
      "\tTotal Violations                   = 147.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.303\n",
      "Violation Counter \t= 58\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.477\n",
      "\tBEST TOTAL VIOLATIONS              = 147.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.741\n",
      "\tTotal Violations                   = 146.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.011\n",
      "Violation Counter \t= 44\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 146.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.427\n",
      "\tTotal Violations                   = 96.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= -0.471\n",
      "Violation Counter \t= 58\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 96.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.164\n",
      "\tTotal Violations                   = 100.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.030\n",
      "Violation Counter \t= 40\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 96.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.486\n",
      "\tTotal Violations                   = 82.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.131\n",
      "Violation Counter \t= 46\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 82.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.244\n",
      "\tTotal Violations                   = 163.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.406\n",
      "Violation Counter \t= 51\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 82.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.582\n",
      "\tTotal Violations                   = 153.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.493\n",
      "Violation Counter \t= 64\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 82.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.080\n",
      "\tTotal Violations                   = 152.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.186\n",
      "Violation Counter \t= 42\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 82.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.551\n",
      "\tTotal Violations                   = 157.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.457\n",
      "Violation Counter \t= 54\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.741\n",
      "\tBEST TOTAL VIOLATIONS              = 82.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.302\n",
      "\tTotal Violations                   = 160.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.95\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "#             transition_rec[:,5] += r #broadcast reward to all states\n",
    "#             decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "#             transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "     # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counterO\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "Train time: 0:21:29.572680\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('Train time: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2000 \t\t 0.03 \t\t 34\n",
      "2001 \t\t 0.04 \t\t 37\n",
      "2002 \t\t -0.13 \t\t 48\n",
      "2003 \t\t 0.02 \t\t 33\n",
      "2004 \t\t 0.13 \t\t 37\n",
      "2005 \t\t 0.28 \t\t 24\n",
      "2006 \t\t -0.08 \t\t 34\n",
      "2007 \t\t 0.19 \t\t 34\n",
      "2008 \t\t 0.38 \t\t 22\n",
      "2009 \t\t 0.12 \t\t 31\n",
      "2010 \t\t 0.21 \t\t 32\n",
      "2011 \t\t 0.34 \t\t 26\n",
      "2012 \t\t 0.41 \t\t 33\n",
      "2013 \t\t 0.43 \t\t 29\n",
      "2014 \t\t 0.12 \t\t 40\n",
      "2015 \t\t 0.02 \t\t 40\n",
      "2016 \t\t 0.17 \t\t 26\n",
      "2017 \t\t 0.29 \t\t 29\n",
      "2018 \t\t 0.27 \t\t 28\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2000,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2000 \t\t 0.03 \t\t 34\n",
      "2001 \t\t 0.04 \t\t 37\n",
      "2002 \t\t -0.13 \t\t 48\n",
      "2003 \t\t 0.02 \t\t 33\n",
      "2004 \t\t 0.13 \t\t 37\n",
      "2005 \t\t 0.28 \t\t 24\n",
      "2006 \t\t -0.08 \t\t 34\n",
      "2007 \t\t 0.19 \t\t 34\n",
      "2008 \t\t 0.38 \t\t 22\n",
      "2009 \t\t 0.12 \t\t 31\n",
      "2010 \t\t 0.21 \t\t 32\n",
      "2011 \t\t 0.34 \t\t 26\n",
      "2012 \t\t 0.41 \t\t 33\n",
      "2013 \t\t 0.43 \t\t 29\n",
      "2014 \t\t 0.12 \t\t 40\n",
      "2015 \t\t 0.02 \t\t 40\n",
      "2016 \t\t 0.17 \t\t 26\n",
      "2017 \t\t 0.29 \t\t 29\n",
      "2018 \t\t 0.27 \t\t 28\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2000,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:23:03.306485\n"
     ]
    }
   ],
   "source": [
    "print('Total runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
