{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 3 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "#     std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "EPSILON = 0.5\n",
      "\n",
      "Iteration 0:  TOKYO, 2008 \n",
      "Average Reward \t\t= -5.432\n",
      "Violation Counter \t= 293\n",
      "EPSILON = 0.75\n",
      "\n",
      "Iteration 1:  TOKYO, 2003 \n",
      "Average Reward \t\t= -5.317\n",
      "Violation Counter \t= 278\n",
      "EPSILON = 0.83\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -2.036\n",
      "Violation Counter \t= 157\n",
      "EPSILON = 0.88\n",
      "\n",
      "Iteration 3:  TOKYO, 2002 \n",
      "Average Reward \t\t= -2.829\n",
      "Violation Counter \t= 182\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 4:  TOKYO, 2003 \n",
      "Average Reward \t\t= -2.505\n",
      "Violation Counter \t= 147\n",
      "EPSILON = 0.92\n",
      "\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.878\n",
      "Violation Counter \t= 103\n",
      "EPSILON = 0.93\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.308\n",
      "Violation Counter \t= 68\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 7:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.153\n",
      "Violation Counter \t= 68\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 8:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.015\n",
      "Violation Counter \t= 68\n",
      "EPSILON = 0.95\n",
      "\n",
      "Iteration 9:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.891\n",
      "Violation Counter \t= 68\n",
      "EPSILON = 0.5\n",
      "\n",
      "Iteration 10:  TOKYO, 2000 \n",
      "Average Reward \t\t= -3.870\n",
      "Violation Counter \t= 240\n",
      "EPSILON = 0.75\n",
      "\n",
      "Iteration 11:  TOKYO, 2007 \n",
      "Average Reward \t\t= -2.148\n",
      "Violation Counter \t= 155\n",
      "EPSILON = 0.83\n",
      "\n",
      "Iteration 12:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.639\n",
      "Violation Counter \t= 116\n",
      "EPSILON = 0.88\n",
      "\n",
      "Iteration 13:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.236\n",
      "Violation Counter \t= 99\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.420\n",
      "Violation Counter \t= 104\n",
      "EPSILON = 0.92\n",
      "\n",
      "Iteration 15:  TOKYO, 2001 \n",
      "Average Reward \t\t= -1.172\n",
      "Violation Counter \t= 85\n",
      "EPSILON = 0.93\n",
      "\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.047\n",
      "Violation Counter \t= 83\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 17:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.029\n",
      "Violation Counter \t= 84\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 18:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.987\n",
      "Violation Counter \t= 75\n",
      "EPSILON = 0.95\n",
      "\n",
      "Iteration 19:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.838\n",
      "Violation Counter \t= 63\n",
      "EPSILON = 0.5\n",
      "\n",
      "Iteration 20:  TOKYO, 2004 \n",
      "Average Reward \t\t= -3.218\n",
      "Violation Counter \t= 236\n",
      "EPSILON = 0.75\n",
      "\n",
      "Iteration 21:  TOKYO, 2008 \n",
      "Average Reward \t\t= -2.108\n",
      "Violation Counter \t= 163\n",
      "EPSILON = 0.83\n",
      "\n",
      "Iteration 22:  TOKYO, 2004 \n",
      "Average Reward \t\t= -1.321\n",
      "Violation Counter \t= 114\n",
      "EPSILON = 0.88\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= -1.630\n",
      "Violation Counter \t= 120\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 24:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.921\n",
      "Violation Counter \t= 84\n",
      "EPSILON = 0.92\n",
      "\n",
      "Iteration 25:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.902\n",
      "Violation Counter \t= 79\n",
      "EPSILON = 0.93\n",
      "\n",
      "Iteration 26:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.410\n",
      "Violation Counter \t= 54\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 27:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.540\n",
      "Violation Counter \t= 72\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 28:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.514\n",
      "Violation Counter \t= 70\n",
      "EPSILON = 0.95\n",
      "\n",
      "Iteration 29:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.442\n",
      "Violation Counter \t= 71\n",
      "EPSILON = 0.5\n",
      "\n",
      "Iteration 30:  TOKYO, 2006 \n",
      "Average Reward \t\t= -4.167\n",
      "Violation Counter \t= 260\n",
      "EPSILON = 0.75\n",
      "\n",
      "Iteration 31:  TOKYO, 2002 \n",
      "Average Reward \t\t= -2.212\n",
      "Violation Counter \t= 175\n",
      "EPSILON = 0.83\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.388\n",
      "Violation Counter \t= 130\n",
      "EPSILON = 0.88\n",
      "\n",
      "Iteration 33:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.227\n",
      "Violation Counter \t= 117\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 34:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.036\n",
      "Violation Counter \t= 107\n",
      "EPSILON = 0.92\n",
      "\n",
      "Iteration 35:  TOKYO, 2002 \n",
      "Average Reward \t\t= -1.152\n",
      "Violation Counter \t= 111\n",
      "EPSILON = 0.93\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.594\n",
      "Violation Counter \t= 80\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 37:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.643\n",
      "Violation Counter \t= 71\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 38:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.690\n",
      "Violation Counter \t= 95\n",
      "EPSILON = 0.95\n",
      "\n",
      "Iteration 39:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.983\n",
      "Violation Counter \t= 102\n",
      "EPSILON = 0.5\n",
      "\n",
      "Iteration 40:  TOKYO, 2006 \n",
      "Average Reward \t\t= -4.388\n",
      "Violation Counter \t= 282\n",
      "EPSILON = 0.75\n",
      "\n",
      "Iteration 41:  TOKYO, 2000 \n",
      "Average Reward \t\t= -2.287\n",
      "Violation Counter \t= 176\n",
      "EPSILON = 0.83\n",
      "\n",
      "Iteration 42:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.340\n",
      "Violation Counter \t= 128\n",
      "EPSILON = 0.88\n",
      "\n",
      "Iteration 43:  TOKYO, 2004 \n",
      "Average Reward \t\t= -1.201\n",
      "Violation Counter \t= 117\n",
      "EPSILON = 0.9\n",
      "\n",
      "Iteration 44:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.855\n",
      "Violation Counter \t= 94\n",
      "EPSILON = 0.92\n",
      "\n",
      "Iteration 45:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.253\n",
      "Violation Counter \t= 115\n",
      "EPSILON = 0.93\n",
      "\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.990\n",
      "Violation Counter \t= 92\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 47:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.919\n",
      "Violation Counter \t= 83\n",
      "EPSILON = 0.94\n",
      "\n",
      "Iteration 48:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.741\n",
      "Violation Counter \t= 85\n",
      "EPSILON = 0.95\n",
      "\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.838\n",
      "Violation Counter \t= 95\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    counter = iteration%10\n",
    "    EPSILON = 0.5*counter/(counter+1) + 0.5#sawtooth learning rate for disruptive learning\n",
    "    print('EPSILON = {:.2}'.format(EPSILON))\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "   \n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.95\n",
      "LR      =  0.0001\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.859\n",
      "Violation Counter \t= 95\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.708\n",
      "\tTotal Violations                   = 469.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2004 \n",
      "Average Reward \t\t= -1.003\n",
      "Violation Counter \t= 103\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.708\n",
      "\tBEST TOTAL VIOLATIONS              = 469.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.510\n",
      "\tTotal Violations                   = 391.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.566\n",
      "Violation Counter \t= 80\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.510\n",
      "\tBEST TOTAL VIOLATIONS              = 391.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.817\n",
      "\tTotal Violations                   = 458.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= -0.620\n",
      "Violation Counter \t= 87\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.510\n",
      "\tBEST TOTAL VIOLATIONS              = 391.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.110\n",
      "\tTotal Violations                   = 426.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.479\n",
      "Violation Counter \t= 79\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.110\n",
      "\tBEST TOTAL VIOLATIONS              = 391.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.707\n",
      "\tTotal Violations                   = 508.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.592\n",
      "Violation Counter \t= 75\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.110\n",
      "\tBEST TOTAL VIOLATIONS              = 391.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.723\n",
      "\tTotal Violations                   = 447.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.921\n",
      "Violation Counter \t= 90\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.110\n",
      "\tBEST TOTAL VIOLATIONS              = 391.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.566\n",
      "\tTotal Violations                   = 361.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.672\n",
      "Violation Counter \t= 81\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.110\n",
      "\tBEST TOTAL VIOLATIONS              = 361.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -1.067\n",
      "\tTotal Violations                   = 585.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2005 \n",
      "Average Reward \t\t= -0.272\n",
      "Violation Counter \t= 61\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -0.110\n",
      "\tBEST TOTAL VIOLATIONS              = 361.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.050\n",
      "\tTotal Violations                   = 218.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.751\n",
      "Violation Counter \t= 82\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 0.050\n",
      "\tBEST TOTAL VIOLATIONS              = 218.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.655\n",
      "\tTotal Violations                   = 521.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.95\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "     # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counterO\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "Train time: 0:20:23.898907\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('Train time: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2000 \t\t -0.33 \t\t 79\n",
      "2001 \t\t -0.34 \t\t 81\n",
      "2002 \t\t -0.49 \t\t 96\n",
      "2003 \t\t -0.28 \t\t 71\n",
      "2004 \t\t -0.61 \t\t 102\n",
      "2005 \t\t -0.33 \t\t 79\n",
      "2006 \t\t -0.04 \t\t 58\n",
      "2007 \t\t -0.45 \t\t 87\n",
      "2008 \t\t -0.33 \t\t 78\n",
      "2009 \t\t -0.27 \t\t 73\n",
      "2010 \t\t -0.53 \t\t 94\n",
      "2011 \t\t -0.59 \t\t 98\n",
      "2012 \t\t -0.7 \t\t 109\n",
      "2013 \t\t -0.74 \t\t 112\n",
      "2014 \t\t -0.71 \t\t 108\n",
      "2015 \t\t -0.53 \t\t 94\n",
      "2016 \t\t -0.35 \t\t 80\n",
      "2017 \t\t -0.44 \t\t 84\n",
      "2018 \t\t -0.74 \t\t 110\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2000,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2000 \t\t -0.33 \t\t 79\n",
      "2001 \t\t -0.34 \t\t 81\n",
      "2002 \t\t -0.49 \t\t 96\n",
      "2003 \t\t -0.28 \t\t 71\n",
      "2004 \t\t -0.61 \t\t 102\n",
      "2005 \t\t -0.33 \t\t 79\n",
      "2006 \t\t -0.04 \t\t 58\n",
      "2007 \t\t -0.45 \t\t 87\n",
      "2008 \t\t -0.33 \t\t 78\n",
      "2009 \t\t -0.27 \t\t 73\n",
      "2010 \t\t -0.53 \t\t 94\n",
      "2011 \t\t -0.59 \t\t 98\n",
      "2012 \t\t -0.7 \t\t 109\n",
      "2013 \t\t -0.74 \t\t 112\n",
      "2014 \t\t -0.71 \t\t 108\n",
      "2015 \t\t -0.53 \t\t 94\n",
      "2016 \t\t -0.35 \t\t 80\n",
      "2017 \t\t -0.44 \t\t 84\n",
      "2018 \t\t -0.74 \t\t 110\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2000,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:21:56.455659\n"
     ]
    }
   ],
   "source": [
    "print('Total runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
